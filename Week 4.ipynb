{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDA1-CE1000: Introduction to Real Estate Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4\n",
    "\n",
    "\n",
    "\n",
    "### Continued Exploration of Causality: Raise the Rent to Lower Vacancy!\n",
    "\n",
    "\n",
    "\n",
    "### Extending the Bivariate Algorithm to the Multivariate Algorithm: CAPM to Fama-French Factor Models (and Beyond)\n",
    "\n",
    "\n",
    "\n",
    "### Model Diagnostics and Their Valid Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Continued Discussion of Empirical Causality\n",
    "\n",
    "* Prior to the 1930s, there were few to none government-sponsored measures of economic activity.\n",
    "    * The US unemployment rate in 1930 [was 25%](https://www.marketwatch.com/story/the-soaring-us-unemployment-rate-could-approach-great-depression-era-levels-2020-04-03).\n",
    "    * In truth, this is an approximation as there were few measures of economic activity at the time.\n",
    "    * This is not to say that **sovereigns** were not economic actors.  [Remarkable research](https://www.bankofengland.co.uk/working-paper/2020/eight-centuries-of-global-real-interest-rates-r-g-and-the-suprasecular-decline-1311-2018) from the Bank of England on long-duration, risk-free interest rates.\n",
    "\n",
    "\n",
    "\n",
    "* As the US and Europe slid into the Great Depression of the 1930's, policy makers lacked basic information.\n",
    "\n",
    "\n",
    "\n",
    "* In the US, the National Income and Product Accounts (NIPA) were born in 1934 and greatly expanded during and after WWII.\n",
    "    * Gross National Product = Consumption + Gov't Spending + Investment + Exports - Imports = $C + G + I + X - M$\n",
    "    * Gross Domestic Product = Consumption + Gov't Spending + Investment = $C + G + I$\n",
    "\n",
    "\n",
    "\n",
    "* At the time, Alfred Cowles established the [Cowles Commission](https://en.wikipedia.org/wiki/Cowles_Foundation) for Research in Economics.\n",
    "\n",
    "\n",
    "\n",
    "* Cowles approach was a probabilistic framework to estimate systems of simultaneous equations to model an economy.  Consider, [GDP](https://en.wikipedia.org/wiki/Multiplier_(economics)):\n",
    "    * $C = C($aggregate output and other things$)$\n",
    "    * $G = \\bar G$, fixed and given\n",
    "    * $I = I($interest rates, changes in consumption and other things$)$\n",
    "    * Your spending is my income and my spending is your income\n",
    "    * Consumption and investment are related via a multiplier\n",
    "\n",
    "\n",
    "\n",
    "* Ultimately Cowles would develop very large scale **macroeconometric models** to examine a host of different economic variables. \n",
    "    * Hard-won gains\n",
    "    * But approach was found to be inadequate for policy evaluation: **Goodhart’s Conjecture** and the **Lucas critique**.\n",
    "    \n",
    "\n",
    "\n",
    "### Goodhart's Conjecture\n",
    "\n",
    "* Goodhart “asserts that any economic relation tends to break down when used for policy purposes.”  [Wickens](https://www.amazon.com/Macroeconomic-Theory-Dynamic-Equilibrium-Approach/dp/0691152861/ref=sr_1_1?dchild=1&keywords=wickens&qid=1591918350&sr=8-1)\n",
    "    * Proposed relationships, economic or otherwise, are not structural in nature.\n",
    "    * Instead they are derived from fundamental behavioral relationships (sometimes called *structural*).\n",
    "    \n",
    "\n",
    "\n",
    "### The Lucas Critique\n",
    "\n",
    "* Lucas (1976) notes that individual decision rules affected by policy are driven by “deep structural parameters.” \n",
    "    * Decision rules and, therefore, decisions are contingent on the state of the system as it is.\n",
    "    * Change the system through policy, change the decision rule.\n",
    "    * Such changes may not be captured in non-structural models.\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "### Experimental Design: Natural Experiments (Freakonomics)\n",
    "\n",
    "> A natural experiment is an empirical study in which individuals (or clusters of individuals) exposed to the experimental and control conditions are determined by nature or by other factors outside the control of the investigators, yet the process governing the exposures arguably resembles random assignment. [Wikipedia](https://en.wikipedia.org/wiki/Natural_experiment)\n",
    "\n",
    "* Examples\n",
    "    * **Natural disasters**\n",
    "        * 1906 S.F. earthquake to examine the impact of stock changes (vacancy) on rent.\n",
    "            * $\\rightarrow$ given the gold standard, the 1906 earthquake arguably **caused** the 1907 Depression, the proximate cause of the 1913 Federal Reserve Act\n",
    "        * Hurricane Sandy and a [potential algorithmic counterfactual](https://github.com/thsavage/Causation/blob/master/Poster.pdf).\n",
    "        * COVID-19 and its [potential impacts](https://www.vox.com/recode/2020/4/14/21211789/coronavirus-office-space-work-from-home-design-architecture-real-estate) on commercial real estate.\n",
    "    * **Lotteries** that truly randomize a group of individuals.  \n",
    "        * Vietnam draft in the US as a means to explore the impacts of education on wages.\n",
    "        * Delay in schooling reduces wages (even if the same level of schooling is achieved).\n",
    "        * Randomized eligibility for mandatory military service in Argentina.  \n",
    "            * Actual conscription increases the likelihood of having a criminal record later in adulthood. \n",
    "            * Possible inference: Delayed entry to the labor market has adverse implications in later labor market outcomes.\n",
    "        * Recent paper on the impact of rent control on quality of housing stock.\n",
    "    * **Jurisdictional boundaries** over which policies are different.\n",
    "        * Different minimum wage laws (New Jersey and Pennsylvania) to examine the impact of minimum wages on employment levels.\n",
    "        * [North v. South Korea at night](https://www.sciencephoto.com/media/108308/view/korea-at-night-satellite-image).\n",
    "    * **Same individuals** faced with exogenous changes.\n",
    "        * Tim in the classroom versus Tim online.\n",
    "        \n",
    "\n",
    "\n",
    "### Big Data: Will the Explosion of Data Generation Help?\n",
    "* Remember the AAPL graph: introduction of the IPhone created the current digital revolution.\n",
    "    * We generate more data in a day today than we created in a year 10 years ago.\n",
    "    * It is the **digital exhaust** of human activity.\n",
    "    * Non-experimental.\n",
    "* Will it help? \n",
    "    * Perhaps, but it depends on understanding algorithms.\n",
    "* In the meantime, let's explore how CRE thinks about these ideas.\n",
    "\n",
    "\n",
    "\n",
    "### A Real Estate Example: Landlords Should Raise the Rent to Lower Their Vacancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with Multiple Features and the Valid Application of Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is trivial to extend the bivariate linear model to a linear model that simultaneously incorporates multiple features.  \n",
    "    * The interpretation of the results of a statistical model that uses multiple features is the same the interpretation of the partial derivative from the calculus of many variables: the effect of a small change in a particular feature on a label (or outcome) **holding all else constant**.  \n",
    "\n",
    "\n",
    "\n",
    "* A model with $K$ features, $x_{ik}$ and label $y_i$:\n",
    "    $y_i=\\sum_{k=1}^Kx_{ik}\\cdot\\beta_k+\\epsilon_i = x_i^\\prime \\beta + \\epsilon_i$\n",
    "\n",
    "\n",
    "\n",
    "* The $K$ features $x_{ik}$ influence the label $y_i$ through the $K$-vector, $\\beta$, which we fit using the linear estimator or sometimes called **multivariate regression**.  As a result, we can use partial differentiation to interpret the results.\n",
    "\n",
    "    ${\\displaystyle \\frac{\\partial E(\\hat y_i)}{\\partial x_{ik}}=\\hat \\beta_k}$\n",
    "\n",
    "\n",
    "\n",
    "* For those interested in history: [Frisch–Waugh–Lovell theorem](https://en.wikipedia.org/wiki/Frisch–Waugh–Lovell_theorem).  One can applied the basic principle from multivariate calculus: Holding everything else constant, what is the impact of a feature on an outcome of interest.\n",
    "\n",
    "**Bottom line is simple**: Fit the linear model with multiple features.  The basic approach to hypothesis testing remains unchanged.  The challenge is the interpretation of the results and the valid application of regression diagnostics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness of Fit: R$^2$\n",
    "\n",
    "* Different statisical computing environments largely produce the same regression ouput, formatted differently.  \n",
    "    * This information is typically used for **regression diagnosgics**.  \n",
    "    * We have discussed regression coefficients and their interpretation, as well as the use of  confidence intervals for hypothesis testing. \n",
    "\n",
    "\n",
    "\n",
    "* The R$^2$ **goodness of fit** metric is a frequently-cited regression diagnostic.  \n",
    "    * If a linear regression uses a constant (which should be included in practice), the R$^2$ is bounded between 0 and 1.  \n",
    "    * It measures the share of the variation in $y$ explained by the variation in the features used in a model.  \n",
    "    * Given this definition, the idea that **bigger is better** tends to prevail.  is the first place that people go to evaluate the quality of the model, which is unwarranted.  \n",
    "\n",
    "\n",
    "\n",
    "> \"However, it can still be challenging to determine what is a good R$^2$ value, and in general, this will depend on the application.  For instance, in certain problems in physics, we may know that the data truly comes from a linear model with a small residual error.  In this case, we would expect to see an R$^2$ value that is extremely close to 1, and a substantially smaller R$^2$ might indicate serious problems with the experiment in which the data were generated.  On the other hand, in typical application in biology, pyschology, marketing and other domains, the linear model is at best an extremely rough approximation to the data, and residual errors due to other unmeasured factors are often very large.  In this setting, we would expect only a very small proportion of the variance in the response to be explained by the predictor, and an R$^2$ value well below 0.1 might be more realistic.\"  \n",
    "\n",
    "> Trevor Hastie, Robert Tibshirani, et al.\n",
    "\n",
    "\n",
    "\n",
    "* Let's consider this this idea by extending CAPM to the Fama-French factor models.\n",
    "    * An exercise in data mining in finance.\n",
    "    \n",
    "    \n",
    "    \n",
    "* Consider the distinction between risk and uncertainty made by [Frank Knight](https://en.wikipedia.org/wiki/Frank_Knight).  The same argument can be applied to the R$^2$ metric when used in isolation.  We do not know the total variance of the system (which is essentially the point being made by Hastie and Tibshirani).\n",
    "    * COVID-19 is example where **Knightian uncertainty** has increased.  As a result, market volatility may, in truth, not have increased. \n",
    "    * In the R example, we have generated data from two systems, one with smaller **total variance**.\n",
    "    * In the R example, we generated the **perfect model**.  (This is not factually correct.)\n",
    "    \n",
    "    \n",
    "    \n",
    "* The valid application of the R$^2$ metric is to compare nested models.  (In time series, we will see another method of optimization, called the **method of maximum likelihood**.  The same argument will prevail there.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
